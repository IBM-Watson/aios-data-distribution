{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Notebook for generating training data distribution and configuring Fairness\n",
    "\n",
    "This notebook analyzes training data and outputs a JSON which contains information related to data distribution and fairness configuration.  In order to use this notebook you need to do the following:\n",
    "\n",
    "1. Read the training data into a pandas dataframe called \"data_df\".  There is sample code below to show how this can be done if the training data is in IBM Cloud Object Storage. \n",
    "2. Edit the below cells and provide the training data and fairness configuration information. \n",
    "3. Run the notebook. It will generate a JSON and a download link for the JSON will be present at the very end of the notebook.\n",
    "4. Download the JSON by clicking on the link and upload it in the IBM AI OpenScale GUI.\n",
    "\n",
    "If you have multiple models (deployments), you will have to repeat the above steps for each model (deployment)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read training data into a pandas data frame\n",
    "\n",
    "The first thing that you need to do is to read the training data into a pandas dataframe called \"data_df\".  Given below is sample code for doing this if the training data is in IBM Cloud Object Storage.  Please edit the below cell and make changes so that you can read your training data from the location where it is stored.  Please ensure that the training data is present in a data frame called \"data_df\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------------------------------------------------------------------------\n",
    "# IBM Confidential\n",
    "# OCO Source Materials\n",
    "# 5900-A3Q, 5737-J33\n",
    "# Copyright IBM Corp. 2018\n",
    "# The source code for this Notebook is not published or other-wise divested of its trade \n",
    "# secrets, irrespective of what has been deposited with the U.S.Copyright Office.\n",
    "# ----------------------------------------------------------------------------------------------------\n",
    "\n",
    "!pip install pandas\n",
    "!pip install ibm-cos-sdk\n",
    "!pip install numpy\n",
    "!pip install pyspark\n",
    "\n",
    "VERSION = 1.2\n",
    "\n",
    "# code to read file in COS to pandas dataframe object\n",
    "import sys\n",
    "import types\n",
    "import pandas as pd\n",
    "from ibm_botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "api_key = \"<API Key>\"\n",
    "resource_instance_id = \"crn:v1:bluemix:public:cloud-object-storage:global:a/111111aaa1a111aa11d111111aa11111:22b22bbb-b22b-22bb-2b22-22b22bB22b2b::\"\n",
    "auth_endpoint = \"https://iam.ng.bluemix.net/oidc/token\"\n",
    "service_endpoint =  \"https://s3-api.dal-us-geo.objectstorage.softlayer.net\"\n",
    "bucket =  \"<Bucket Name>\"\n",
    "file_name= \"<File Name>\"\n",
    "\n",
    "cos_client = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id=api_key,\n",
    "    ibm_auth_endpoint=auth_endpoint,\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url=service_endpoint)\n",
    "\n",
    "body = cos_client.get_object(Bucket=bucket,Key=file_name)['Body']\n",
    "\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n",
    "\n",
    "data_df = pd.read_csv(body)\n",
    "data_df.head()\n",
    "\n",
    "#Print columns from data frams\n",
    "#print(\"column names:{}\".format(list(data_df.columns.values)))\n",
    "\n",
    "# Uncomment following 2 lines if you want to read training data from local CSV file when running through local Jupyter notebook\n",
    "#data_df = pd.read_csv(\"<FULLPATH_TO_CSV_FILE>\")\n",
    "#data_df.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Data and Fairness Configuration Information\n",
    "\n",
    "Please provide information about the training data which is used to train the model.  In order to explain the configuration better, let us first consider an example of a Loan Processing Model which is trying to predict whether a person should get a loan or not. The training data for such a model will potentially contain the following columns: Credit_History, Monthly_salary, Applicant_Age, Loan_amount, Gender, Marital_status, Approval.  The \"Approval\" column contains the target field (label column or class label) and it can have the following values: \"Loan Granted\", \"Loan Denied\" or \"Loan Partially Granted\".  In this model we would like to ensure that the model is not biased against Gender=Female or Gender=Transgender.  We would also like to ensure that the model is not biased against the age group 15 to 30 years or age group 61 to 120 years. \n",
    "\n",
    "For the above model, the configuration information that we need to provide is:\n",
    "\n",
    "- class_label:  This is the name of the column in the training data dataframe (data_df) which contains the target field (also known as label column or the class label).  For the Loan Processing Model it would be \"Approval\".\n",
    "- feature_columns: This is a comma separated list of column names which contain the feature column names (in the training data dataframe data_df).  For the Loan Processing model this would be: [\"Credit_History\", \"Monthly_salary\", \"Applicant_Age\", \"Loan_amount\", \"Gender\", \"Marital_status\"]\n",
    "- categorical_columns: The list of column names (in data_df) which contain categorical values.  This should also include those columns which originally contained categorical values and have now been converted to numeric values. E.g., in the Loan Processing Model, the Marital_status column originally could have values: Single, Married, Divorced, Separated, Widowed.  These could have been converted to numeric values as follows: Single -> 0, Married -> 1, Divorced -> 2, Separated -> 3 and Widowed -> 4.  Thus the training data will have numeric values.  Please identify such columns as categorical.  Thus the list of categorical columns for the Loan Processing Model will be Credit_History, Gender and Marital_status. \n",
    "\n",
    "For the Loan Processing Model, this information will be provided as follows:\n",
    "\n",
    "training_data_info = { <br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\"class_label\": \"Approval\",   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\"feature_columns\": [\"Credit_History\", \"Monthly_salary\", \"Applicant_Age\", \"Loan_amount\", \"Gender\", \"Marital_status\"],    \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;\"categorical_columns\": [\"Credit_History\",\"Gender\",\"Marital_status\"]   \n",
    "    }  \n",
    "    \n",
    "  **Note:** Please note that categorical columns selected should be subset of feature columns. If there are no categorical columns among the feature columns selected , please set \"categorical_columns as [] or None\"\n",
    "\n",
    "Please edit the next cell and provide the above information for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_data_info = {\n",
    "    \"class_label\": \"<EDIT THIS>\",\n",
    "    \"feature_columns\": [\"<EDIT THIS>\"],\n",
    "    \"categorical_columns\": [\"<EDIT THIS>\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the Model Type\n",
    "\n",
    "In the next cell, specify the type of your model.  If your model is a binary classification model, then set the type to \"binary\". If it is a multi-class classifier then set the type to \"multiclass\". If it is a regression model (e.g., Linear Regression), then set it to \"regression\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set model_type. Acceptable values are:[\"binary\",\"multiclass\",\"regression\"]\n",
    "model_type = \"binary\"\n",
    "#model_type = \"multiclass\"\n",
    "#model_type = \"regression\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the Fairness Configuration\n",
    "\n",
    "You need to provide the following information for the fairness configuration: \n",
    "\n",
    "- fairness_attributes:  These are the attributes on which you wish to monitor fairness. In the Loan Processing Model, we wanted to ensure that the model is not baised against people of specific age group and people belonging to a specific gender.  Hence \"Applicant_Age\" and \"Gender\" will be the fairness attributes for the Loan Processing Model.\n",
    "- type: The data type of the fairness attribute (e.g., float or int or double)\n",
    "- minority:  The minority group for which we want to ensure that the model is not biased.  For the Loan Processing Model we wanted to ensure that the model is not biased against people in the age group 15 to 30 years & 61 to 120 years as well as people with Gender = Female or Gender = Transgender.  Hence the minority group for the fairness attribute \"Applicant_Age\" will be [15,30] and [61,120] and the minority group for fairness attribute \"Gender\" will be: \"Female\", \"Transgender\".  \n",
    "- majority: The majority group for which the model might be biased towards.  For the Loan Processing Model, the majority group for the fairness attribute \"Applicant_Age\" will be [31,60], i.e., all the ages except the minority group.  For the fairness attribute \"Gender\" the majority group will be: \"Male\".  \n",
    "- threshold:  The fairness threshold beyond which the Model is considered to be biased.  For the Loan Processing Model, let us say that the Bank is willing to tolerate the fact that Female and Transgender applicants will get upto 20% lesser approved loans than Males.  However, if the percentage is more than 20% then the Loan Processing Model will be considered biased.  E.g., if the percentage of approved loans for Female or Transgender applicants is say 25% lesser than those approved for Male applicants then the Model is to be considered as acting in a biased manner.  Thus for this scenario, the Fairness threshold will be 80 (100-20) (this is represented as a value normalized to 1, i.e., 0.8).  \n",
    "\n",
    "The fairness attributes for Loan Processing Model will be specified as:\n",
    "\n",
    "fairness_attributes = [  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"feature\": \"Applicant_Age\",   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"type\" : \"int\",   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"majority\": [ [31,60] ],   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"minority\": [ [15, 30], [61,120] ],  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"threshold\" : 0.8  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;},  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;{  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"feature\": \"Gender\",   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"type\" : \"string\",   \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"majority\": [\"Male\"],  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"minority\": [\"Female\", \"Transgender\"],  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"threshold\" : 0.8  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;}  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;]  \n",
    "\n",
    "Please edit the next cell and provide the fairness configuration for your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fairness_attributes = [{\n",
    "                           \"type\" : \"<DATA_TYPE>\", #data type of the column eg: float or int or double\n",
    "                           \"feature\": \"<COLUMN_NAME>\", \n",
    "                           \"majority\": [\n",
    "                               [X, Y] # range of values for column eg: [31, 45] for int or [31.4, 45.1] for float\n",
    "                           ],\n",
    "                           \"minority\": [\n",
    "                               [A, B], # range of values for column eg: [10, 15] for int or [10.5, 15.5] for float\n",
    "                               [C, D]   # range of values for column eg: [80, 100] for int or [80.0, 99.9] for float                    \n",
    "                           ],\n",
    "                           \"threshold\": <VALUE> #such that 0<VALUE<=1. eg: 0.8\n",
    "                       }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the Favorable and Unfavorable class values\n",
    "\n",
    "The second part of fairness configuration is about the favourable and unfavourable class values.  Recall that in the case of Loan Processing Model, the target field (label column or class label) can have the following values: \"Loan Granted\", \"Loan Denied\" and \"Loan Partially Granted\".  Out of these values \"Loan Granted\" and \"Loan Partially Granted\" can be considered as being favorable and \"Loan Denied\" is unfavorable.  In other words in order to measure fairness, we need to know the target field values which can be considered as being favourable and those values which can be considered as unfavourable.  \n",
    "\n",
    "For the Loan Prediction Model, the values can be specified as follows:\n",
    "\n",
    "parameters = {  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"favourable_class\" :  [ \"Loan Granted\", \"Loan Partially Granted\" ],  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"unfavourable_class\": [ \"Loan Denied\" ]  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;}  \n",
    "\n",
    "In case of a regression models, the favourable and unfavourable classes will be ranges.  For example, for a model which predicts medicine dosage, the favorable outcome could be between 80 ml to 120 ml or between 5 ml to 20 ml whereas unfavorable outcome will be values between 21 ml to 79ml.  For such a model, the favorable and unfavorable values will be specified as follows:\n",
    "     \n",
    "parameters = {  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"favourable_class\" :  [ [5, 20], [80, 120] ],  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;\"unfavourable_class\": [ [21, 79] ]  \n",
    "&nbsp;&nbsp;&nbsp;&nbsp;}  \n",
    "\n",
    "Please edit the next cell to provide information about your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For classification models use the below.\n",
    "parameters = {\n",
    "        \"favourable_class\" :  [ \"<EDIT THIS>\", \"<EDIT THIS>\" ],\n",
    "        \"unfavourable_class\": [ \"<EDIT THIS>\" ]\n",
    "    }\n",
    "# For regression models use the below.  Delete the entry which is not required.\n",
    "parameters = {\n",
    "        \"favourable_class\" :  [ [<EDIT THIS>, <EDIT THIS>], [<EDIT THIS>,<EDIT THIS>] ],\n",
    "        \"unfavourable_class\": [ [<EDIT THIS>, <EDIT THIS>] ]\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Specify the number of records which should be processed for Fairness\n",
    "\n",
    "The final piece of information that needs to be provided is the number of records (min_records) that should be used for computing the fairness. Fairness checks runs hourly.  If min_records is set to 5000, then every hour fairness checking will pick up the last 5000 records which were sent to the model for scoring and compute the fairness on those 5000 records.  Please note that fairness computation will not start till the time that 5000 records are sent to the model for scoring.\n",
    "\n",
    "If we set the value of \"min_records\" to a small number, then fairness computation will get influenced by the scoring requests sent to the model in the recent past. In other words, the model might be flagged as being biased if it is acting in a biased manner on the last few records, but overall it might not be acting in a biased manner.  On the other hand, if the \"min_records\" is set to a very large number, then we will not be able to catch model bias quickly. Hence the value of min_records should be set such that it is neither too small or too large.\n",
    "\n",
    "Please updated the next cell to specify a value for min_records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_records = <Minimum number of records to be considered for preforming scoring>\n",
    "min_records = <EDIT THIS>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End of Input \n",
    "\n",
    "You need not edit anything beyond this point.  Run the notebook and go to the very last cell.  There will be a link to download the JSON file (called: \"Download training data distribution JSON file\").  Download the file and upload it using the IBM AI OpenScale GUI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameter check\n",
    "if model_type != \"regression\":\n",
    "    favourable_class = parameters.get(\"favourable_class\")\n",
    "    if (favourable_class is not None):\n",
    "        if not(isinstance(favourable_class,(list,tuple))):\n",
    "            raise Exception(\"'favourable_class' in parameters must be a list of values\")\n",
    "        if isinstance(favourable_class[0],(list,tuple)):\n",
    "            raise Exception(\"'favourable_class' in parameters should not be a list of lists. It should be a list of values.\")\n",
    "    \n",
    "    unfavourable_class = parameters.get(\"unfavourable_class\")\n",
    "    if (unfavourable_class is not None):\n",
    "        if not(isinstance(unfavourable_class,(list,tuple))):\n",
    "            raise Exception(\"'unfavourable_class' in parameters must be a list of values\")\n",
    "        if isinstance(unfavourable_class[0],(list,tuple)):\n",
    "            raise Exception(\"'unfavourable_class' in parameters should not be a list of lists. It should be a list of values.\")\n",
    "    \n",
    "else:\n",
    "    favourable_class = parameters.get(\"favourable_class\")\n",
    "    if(favourable_class is None):\n",
    "        raise Exception(\"'favourable_class' values are required in parameters in case of regression model.\")\n",
    "    else:\n",
    "        if not(isinstance(favourable_class[0],(list,tuple))):\n",
    "            raise Exception(\"'favourable class' in parametrs must be a list of lists\")\n",
    "    \n",
    "    unfavourable_class = parameters.get(\"unfavourable_class\")\n",
    "    if(unfavourable_class is None):\n",
    "        raise Exception(\"'unfavourable_class' values are required in parameters in case of regression model.\")\n",
    "    else:\n",
    "        if not(isinstance(unfavourable_class[0],(list,tuple))):\n",
    "            raise Exception(\"'unfavourable class' in parametrs must be a list of lists\")\n",
    "    \n",
    "# Existence check    \n",
    "if \"class_label\" not in training_data_info:\n",
    "        raise Exception(\"'class_label' attributes in missing in 'training_data_info' input\")\n",
    "if \"feature_columns\" not in training_data_info:\n",
    "        raise Exception(\"'feature_columns' attributes in missing in 'training_data_info' input\")\n",
    "if \"categorical_columns\" not in training_data_info:\n",
    "        raise Exception(\"'categorical_columns' attributes in missing in 'training_data_info' input\") \n",
    "\n",
    "if type(training_data_info.get(\"feature_columns\")) is not list:\n",
    "    raise Exception(\"'feature_columns' should be a list of values\")\n",
    "if type(training_data_info.get(\"categorical_columns\")) is not list:\n",
    "    raise Exception(\"'categorical_columns' should be a list of values\")\n",
    "    \n",
    "if not training_data_info.get(\"feature_columns\"):\n",
    "    raise Exception(\"'feature_columns' should not be none or empty list\")\n",
    "    \n",
    "#Verify existence of feature columns in training data\n",
    "feature_columns = training_data_info.get(\"feature_columns\")\n",
    "if feature_columns is None or len(feature_columns) == 0:\n",
    "    raise Exception(\"'feature_columns' should not be empty\")\n",
    "    \n",
    "columns_from_data_frame = list(data_df.columns.values)\n",
    "check_feature_column_existence = list(set(feature_columns) - set(columns_from_data_frame))\n",
    "if len(check_feature_column_existence) > 0:\n",
    "    raise Exception(\"Feature columns missing in training data.Details:{}\".format(check_feature_column_existence))\n",
    "\n",
    "    \n",
    "#Verify existence of  categorical columns in feature columns\n",
    "categorical_columns = training_data_info.get(\"categorical_columns\")\n",
    "if categorical_columns is not None and len(categorical_columns) > 0:\n",
    "    check_cat_col_existence = list(set(categorical_columns) - set(feature_columns))\n",
    "    if len(check_cat_col_existence) > 0:\n",
    "        raise Exception(\"'categorical_columns' should be subset of feature columns.Details:{}\".format(check_cat_col_existence))\n",
    "            \n",
    "# Input validations\n",
    "for fea in fairness_attributes:\n",
    "    if \"feature\" not in fea:\n",
    "        raise Exception(\"'feature' attributes in missing in 'fairness_attributes' input\")\n",
    "    if \"majority\" not in fea:\n",
    "        raise Exception(\"'majority' attributes in missing in 'fairness_attributes' input\")\n",
    "    if \"minority\" not in fea:\n",
    "        raise Exception(\"'minority' attributes in missing in 'fairness_attributes' input\" )   \n",
    "        \n",
    "acceptable_model_types = [\"binary\",\"multiclass\",\"regression\"]\n",
    "if model_type not in acceptable_model_types:\n",
    "    raise Exception (\"Invalid model type. Acceptable values are:\"+acceptable_model_types)\n",
    "                        \n",
    "if model_type==\"regression\":\n",
    "    if \"favourable_class\" not in parameters:\n",
    "        raise Exception(\"'favourable_class' attributes in missing in 'parameters' input\")\n",
    "    if \"unfavourable_class\" not in parameters:\n",
    "        raise Exception(\"'unfavourable_class' attributes in missing in 'parameters' input\") \n",
    "      \n",
    "fairness_attributes_list = []\n",
    "for fea in fairness_attributes:\n",
    "    fairness_attributes_list.append(fea[\"feature\"])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell contains the methods for validating the fairness attributes for overlapping majority/minority ranges and threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_numeric_attr(value, type, feature):\n",
    "    invalid_value = False\n",
    "    if len(value) != 2:\n",
    "        invalid_value = True\n",
    "    if not invalid_value:\n",
    "        for val in value:\n",
    "            start = value[0]\n",
    "            end = value[1]\n",
    "            if start > end:\n",
    "                raise Exception(\"Invalid range: The numerical range for {0} value of the attribute '{1}' is incorrect, start value of range must be less than the end value.\".format(type,feature[\"feature\"]))\n",
    "    if invalid_value:\n",
    "        error_msg = \"Invalid syntax: The {0} value for the numerical attribute '{1}' must be specified as a list of ranges. Range format: [<begin_value>,<end_value>], Example: [[25,50],[60,75]]\".format(type, feature)\n",
    "        raise Exception(error_msg)\n",
    "\n",
    "def validate_maj_min(feature, maj_min,type):\n",
    "    if maj_min is None or maj_min == '' or maj_min == []:\n",
    "        error_msg = \"Missing required field: You haven't specified {0} value for the feature '{1}'.\".format(type, feature)\n",
    "        raise Exception(error_msg)\n",
    "    if not isinstance(maj_min, list):\n",
    "        error_msg = \"Invalid syntax: The {0} value for feature '{1}' must be specified as a list of categorical values or numerical ranges.\".format(type, feature)\n",
    "        raise Exception(error_msg)\n",
    "    for value in maj_min:\n",
    "        if isinstance(value, list):\n",
    "            validate_numeric_attr(value, type, feature)\n",
    "        elif not isinstance(value, str):\n",
    "            error_msg = \"Invalid syntax: The {0} value for feature '{1}' must be specified as a list of categorical values or numerical ranges.\".format(type, feature)\n",
    "            raise Exception(error_msg)\n",
    "        else:\n",
    "            if value.strip() == '':\n",
    "                error_msg = \"Value of {0} can not be empty.\".format(type)\n",
    "                raise Exception(error_msg)\n",
    "\n",
    "def validate_threshold(feature, threshold):\n",
    "    if threshold is None or threshold == '':\n",
    "        error_msg = \"Missing required field: You haven't specified any threshold value for the feature '{0}'.\".format(feature)\n",
    "        raise Exception(error_msg)\n",
    "    if not isinstance(threshold, float) and not isinstance(threshold, int):\n",
    "        error_msg = \"Invalid type: only numerical values are supported for threshold.\"\n",
    "        raise Exception(error_msg)\n",
    "    if threshold <= 0 or threshold > 1:\n",
    "        error_msg = \"The threshold value provided is invalid, it must be in range 0 < threshold <=1\"\n",
    "        raise Exception(error_msg)\n",
    "\n",
    "def validate_feature(feature):\n",
    "    feature_name = feature.get('feature')\n",
    "    if feature_name is None or feature_name == '':\n",
    "        error_msg = \"Missing required field: You haven't specified the feature name.\"\n",
    "        raise Exception(error_msg)\n",
    "    majority = feature.get('majority')\n",
    "    validate_maj_min(feature_name, majority, 'majority')\n",
    "    minority = feature.get('minority')\n",
    "    validate_maj_min(feature_name, minority, 'minority')\n",
    "    for min_value in minority:\n",
    "        #if attribute is categorical, same value can not be specified in both majority and minority\n",
    "        if type(min_value) != type(majority[0]):\n",
    "            error_msg = \"Type mismatch: The data types of majority and minority for feature '{0}' are not matching.\".format(feature_name)\n",
    "            raise Exception(error_msg)\n",
    "        if isinstance(min_value, str):\n",
    "            if min_value in majority:\n",
    "                error_msg = \"Same value can not be specified as both majority and minority.\"\n",
    "                raise Exception(error_msg)\n",
    "        else:\n",
    "            min_start = min_value[0]\n",
    "            min_end = min_value[1]\n",
    "            for maj_value in majority:\n",
    "                maj_start = maj_value[0]\n",
    "                maj_end = maj_value[1]\n",
    "                if (min_start >= maj_start and min_start <= maj_end) or (min_end >= maj_start and min_end <= maj_end) or (maj_start >= min_start and maj_start <= min_end) or (maj_end >= min_start and maj_end <= min_end):\n",
    "                    error_msg = \"The ranges you specified for the minority and majority values overlap.\"\n",
    "                    raise Exception(error_msg)\n",
    "    threshold = feature.get('threshold')\n",
    "    validate_threshold(feature_name, threshold)\n",
    "\n",
    "for feature in fairness_attributes:\n",
    "    validate_feature(feature)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following cell contain code to generate schema information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell-3 : Responsible for generating schema from training data \n",
    "\n",
    "import numpy as np\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def generate_training_schema(payload_df,feature_columns,categorical_columns=None):\n",
    "    spark = SparkSession.builder.appName('pandasToSparkDF').getOrCreate()\n",
    "    df = spark.createDataFrame(payload_df)\n",
    "    sc = df.schema\n",
    "    fields = []\n",
    "    for f in sc:\n",
    "        field = f.jsonValue()\n",
    "        column = field[\"name\"]\n",
    "\n",
    "        if column in feature_columns:\n",
    "            field[\"metadata\"][\"modeling_role\"] = \"feature\"\n",
    "\n",
    "            #Set categorical column in input schema\n",
    "        if categorical_columns is not None:\n",
    "            if column in categorical_columns:\n",
    "                field[\"metadata\"][\"measure\"] = \"discrete\"\n",
    "\n",
    "        fields.append(field)\n",
    "\n",
    "    training_data_schema = {}\n",
    "    training_data_schema[\"type\"] = \"struct\"\n",
    "    training_data_schema[\"fields\"] = fields\n",
    "    \n",
    "    return training_data_schema\n",
    "\n",
    "feature_columns = training_data_info.get(\"feature_columns\")\n",
    "categorical_columns = training_data_info.get(\"categorical_columns\")\n",
    "training_schema = generate_training_schema(data_df,feature_columns,categorical_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following cell contains methods defined for generating training data distribution :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell-4: Define the function for computing distribution\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "# Licensed Materials - Property of IBM\n",
    "# \n",
    "# (C) Copyright IBM Corp. 2018    All Rights Reserved.\n",
    "# US Government Users Restricted Rights - Use, duplication or disclosure\n",
    "# restricted by GSA ADP Schedule Contract with IBM Corp.\n",
    "# -----------------------------------------------------------------------------\n",
    "import math\n",
    "import random\n",
    "import numpy\n",
    "\n",
    "def get_data_types(val):\n",
    "     \n",
    "    is_numeric = False\n",
    "    is_int = False\n",
    "    is_float = False\n",
    "    try:\n",
    "        float(val)\n",
    "        is_numeric = True\n",
    "        is_float = True\n",
    "    except ValueError:\n",
    "        pass\n",
    " \n",
    "    try:\n",
    "        import unicodedata\n",
    "        unicodedata.numeric(val)\n",
    "        is_numeric = True\n",
    "        is_float = True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        int(val)\n",
    "        is_numeric = True\n",
    "        is_int = True\n",
    "    except ValueError:\n",
    "        pass\n",
    "           \n",
    "    return is_numeric, is_float, is_int\n",
    "\n",
    "def getFrequencyTable(protected_attr_col, class_col):\n",
    "    #Example:{Male: { class_outcome_1:28,class_outcome_2:29,class_outcome_3:50 }, Female: {class_outcome_1:28,class_outcome_2:29,class_outcome_3:50}}\n",
    "    frequency_map={} \n",
    "    #class_label_types = set()\n",
    "    str_flag=False\n",
    "    other_flag=False\n",
    "    nan_num=0\n",
    "    for index,protected_attr_value in enumerate(protected_attr_col):\n",
    "        \n",
    "        #remove initial whitespaces\n",
    "        if type(protected_attr_value) is str:\n",
    "            protected_attr_value=protected_attr_value.lstrip()\n",
    "        else:\n",
    "            #remove nan\n",
    "            if math.isnan(protected_attr_value):\n",
    "                nan_num=nan_num+1\n",
    "                continue\n",
    "        \n",
    "        class_value=class_col[index]\n",
    "        if type(class_value) is str:\n",
    "            class_value = class_value.lstrip()\n",
    "            if class_value.isdigit():\n",
    "                other_flag=True\n",
    "            else:\n",
    "                str_flag=True\n",
    "        else:\n",
    "            #remove nan\n",
    "            if math.isnan(class_value):\n",
    "                nan_num=nan_num+1\n",
    "                continue \n",
    "            else:\n",
    "                other_flag=True\n",
    "        \n",
    "            \n",
    "        #Example is Yes        \n",
    "        #Update frequency table               \n",
    "        #checking if frequency map has value for this protected_attribute value. If Male already exists in the freq map\n",
    "        if protected_attr_value in frequency_map:\n",
    "            \n",
    "            #get the dictionary for counts for different class values for this protected attribute value\n",
    "            freq_count_dict=frequency_map[protected_attr_value]\n",
    "            \n",
    "            #check if this particular  counts for this particular class_value already exists: Example {Yes:50}\n",
    "            if class_value in freq_count_dict:\n",
    "                counts_for_class_value=freq_count_dict[class_value]\n",
    "                counts_for_class_value=counts_for_class_value+1\n",
    "                \n",
    "                #Update counts for this particular class in the frequency count \n",
    "                freq_count_dict.update({class_value:counts_for_class_value})\n",
    "                #Update the final map of freq\n",
    "                frequency_map.update({protected_attr_value:freq_count_dict})\n",
    "            else:\n",
    "                counts_for_class_value=1  \n",
    "                freq_count_dict.update({class_value:counts_for_class_value})\n",
    "                frequency_map.update({protected_attr_value:freq_count_dict})            \n",
    "            \n",
    "        else:\n",
    "            #This protected attribute does not exist in frequency map\n",
    "            counts_for_class_value=1\n",
    "            freq_count_dict={class_value:1}             \n",
    "            frequency_map.update({protected_attr_value:freq_count_dict})\n",
    "    #Log a warning if class label column contains mixed data\n",
    "    if str_flag and other_flag:\n",
    "        print(\"Class label columns contains mixed data\")       \n",
    "    return frequency_map  \n",
    "\n",
    "def modify_distribution(attributes_dataset,protected_attribute,class_dataset):\n",
    "        \n",
    "        #protected attribute column\n",
    "        extracted_column = attributes_dataset[protected_attribute].tolist()       \n",
    "        #class label column\n",
    "        labels=class_dataset.tolist()        \n",
    "        #creating the frequency map for attribute and concerned favourabe class\n",
    "        frequency_map=getFrequencyTable(extracted_column,labels)\n",
    "        return frequency_map\n",
    "    \n",
    "def getDistribution(dataset,inputs): \n",
    "    \n",
    "    distribution_map={}\n",
    "    attributes_dataset = dataset[inputs['fairness_attributes']]\n",
    "    class_dataset = dataset[inputs['class_label']]\n",
    "\n",
    "    # Check if the class_label column is numerical or categorical\n",
    "    class_labels = sorted(class_dataset.tolist())\n",
    "    is_column_numeric = False\n",
    "    count_numeric = 0\n",
    "    row_num = class_dataset.shape[0]\n",
    "    \n",
    "    if row_num<1000:\n",
    "        sample_size = row_num\n",
    "    else:\n",
    "        sample_size = 1000\n",
    "    \n",
    "    for i in range(sample_size):\n",
    "        class_label = class_labels[random.randint(0,row_num-1)]\n",
    "        is_numeric, is_float, is_int = get_data_types(class_label)\n",
    "        if is_numeric or is_float or is_int :    \n",
    "            count_numeric+=1\n",
    "\n",
    "    if count_numeric > (sample_size-count_numeric):\n",
    "        is_column_numeric = True\n",
    "\n",
    "    #Get the additional info for class_label\n",
    "    if  is_numeric:\n",
    "        min = class_labels[0]\n",
    "        for pos in range(1,row_num): \n",
    "            is_numeric, is_float, is_int = get_data_types(class_labels[-pos])\n",
    "            if is_numeric or is_float or is_int :\n",
    "                max = class_labels[-pos]\n",
    "                break\n",
    "        distinct_class_label_list = [min,max]\n",
    "\n",
    "    else:\n",
    "        distinct_class_label_values = set()\n",
    "        for class_label in class_labels:\n",
    "            is_numeric, is_float, is_int = get_data_types(class_label)\n",
    "            if not(is_numeric or is_float or is_int):\n",
    "                distinct_class_label_values.add(class_label)\n",
    "        distinct_class_label_list = list(distinct_class_label_values)\n",
    "    \n",
    "    for protected_attribute in inputs['fairness_attributes']: \n",
    "        distribution_map[protected_attribute]=modify_distribution(attributes_dataset,protected_attribute,class_dataset)\n",
    "    \n",
    "    return distribution_map,distinct_class_label_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following cell contains code for generating training data distribution json for fairness. It uses the methods defined in above cell and expects pandas dataframe as input :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell-5: Define the function for computing distribution\n",
    "\n",
    "import math\n",
    "import json\n",
    "import numpy\n",
    "import base64\n",
    "import copy\n",
    "import ast\n",
    "import io,sys\n",
    "import datetime\n",
    "import re\n",
    "import random\n",
    "from IPython.display import HTML\n",
    "\n",
    "total_rows = data_df.shape[0]\n",
    "print(\"Total Rows retrieved \" + str(total_rows))\n",
    "\n",
    "fairness_params = { \"class_label\" : training_data_info[\"class_label\"],\n",
    "                    \"fairness_attributes\" : fairness_attributes_list }\n",
    "data,distinct_class_label_values = getDistribution(data_df,fairness_params)\n",
    "########################################################\n",
    "\n",
    "from datetime import datetime,timedelta\n",
    "import logging,time\n",
    "\n",
    "import json\n",
    "import numpy\n",
    "import math\n",
    "import datetime\n",
    "import copy\n",
    "import ast\n",
    "import numpy as np\n",
    "#from numba.tests.test_conversion import addition\n",
    "\n",
    "\n",
    "def computeTrainingDataDistribution():\n",
    "      \n",
    "    try:    \n",
    "        \n",
    "        class_label = training_data_info[\"class_label\"]\n",
    "        \n",
    "        fairness_params = { \"class_label\" : training_data_info[\"class_label\"],\n",
    "                            \"fairness_attributes\" : fairness_attributes_list }\n",
    "        \n",
    "        data_frame, feature_data_types = cleanPayloadData(data_df,fairness_params)\n",
    "        \n",
    "        total_rows = data_frame.shape[0]\n",
    "        print(\"Total Rows retrieved \" + str(total_rows))\n",
    "        \n",
    "        distribution_data = []   \n",
    "        favourable_unfavourable_class = []\n",
    "        if model_type is not None and model_type==\"regression\":\n",
    "            \n",
    "            favourable_class = parameters[\"favourable_class\"]\n",
    "            unfavourable_class = parameters[\"unfavourable_class\"]\n",
    "            #Combine the favourable and unfavourable ranges \n",
    "            favourable_unfavourable_class.extend(parameters['favourable_class'])\n",
    "            favourable_unfavourable_class.extend(parameters['unfavourable_class'])\n",
    "            distribution_data = compute_regression_training_distribution(data,  fairness_attributes_list, favourable_unfavourable_class, feature_data_types)\n",
    "        else:\n",
    "            distribution_data =  compute_training_distribution(data_frame, data, training_data_info, fairness_attributes, feature_data_types ) \n",
    "\n",
    "        distinct_class_label_feature = {}\n",
    "        distinct_class_label_feature[\"attribute\"] = fairness_params[\"class_label\"]\n",
    "        distinct_class_label_feature[\"is_class_label\"] = True\n",
    "        if model_type is None or model_type!=\"regression\":\n",
    "            temp = distinct_class_label_values[0]\n",
    "            if type(temp) is int or type(temp) is float:\n",
    "                if(distinct_class_label_values[0]>distinct_class_label_values[1]):\n",
    "                    distinct_class_label_feature[\"min\"] = distinct_class_label_values[1]\n",
    "                    distinct_class_label_feature[\"max\"] = distinct_class_label_values[0]\n",
    "                else:\n",
    "                    distinct_class_label_feature[\"min\"] = distinct_class_label_values[0]\n",
    "                    distinct_class_label_feature[\"max\"] = distinct_class_label_values[1]\n",
    "            else:\n",
    "                distinct_class_label_feature[\"distinct_values\"] = distinct_class_label_values        \n",
    "        else:\n",
    "            vList = []\n",
    "            vList.extend(favourable_unfavourable_class)\n",
    "            distinct_class_label_feature[\"distinct_values\"] = vList\n",
    "        \n",
    "        distribution_data.append(distinct_class_label_feature)    \n",
    "            \n",
    "        return distribution_data \n",
    "                    \n",
    "    except Exception as exc:\n",
    "        raise exc\n",
    "\n",
    "\n",
    "def compute_regression_training_distribution(data, fairness_attributes, favourable_classes,feature_data_types):\n",
    "    distribution_data = []    \n",
    "    distinct_data = {}\n",
    "    for fairness_attribute in fairness_attributes[:]:\n",
    "        keys = sorted(data[fairness_attribute].keys())\n",
    "\n",
    "        is_numeric, is_float, is_int = get_data_types(keys[0])\n",
    "        \n",
    "        if(feature_data_types[fairness_attribute]==\"categorical\"):\n",
    "            is_float = is_int = is_numeric = False\n",
    "\n",
    "        key_values = None\n",
    "        min_value = None\n",
    "        max_value = None\n",
    "        if len(keys)>0:\n",
    "            if is_numeric:\n",
    "                if is_float:\n",
    "                    key_values = sorted(list(map(float, keys)))\n",
    "                else:\n",
    "                    key_values = sorted(list(map(int, keys)))    \n",
    "\n",
    "                min_value = key_values[0]\n",
    "                max_value = key_values[len(key_values)-1]\n",
    "\n",
    "        if not is_numeric:\n",
    "            distinct_data[fairness_attribute] = keys\n",
    "\n",
    "        feature = {}\n",
    "        feature[\"attribute\"] = fairness_attribute\n",
    "        if is_numeric is True:\n",
    "            feature[\"min\"] = min_value\n",
    "            feature[\"max\"] = max_value\n",
    "        else:    \n",
    "            feature[\"distinct_values\"] = keys    \n",
    "\n",
    "        class_label_values = []\n",
    "        for key1, value1 in sorted(data[fairness_attribute].items()):\n",
    "            value_array = {}\n",
    "            value_array[\"label\"] = key1\n",
    "            sortedValues = sorted(value1.items())\n",
    "            ranges = {}\n",
    "            for listValue in favourable_classes[:]:\n",
    "                ranges[tuple(listValue)] = 0\n",
    "\n",
    "            for key2, value2 in sortedValues:\n",
    "                for rKey, rValue in ranges.items():\n",
    "                    range_start = rKey[0]\n",
    "                    range_end = rKey[1]\n",
    "                    if key2>=range_start and key2<=range_end:\n",
    "                        ranges[rKey] = ranges[rKey] + 1\n",
    "                        break\n",
    "            range_items = ranges.items()  \n",
    "            range_to_delete = []     \n",
    "            for k, v in range_items:\n",
    "                if ranges[k]==0:\n",
    "                    range_to_delete.append(k)\n",
    "            for x in range_to_delete[:]:\n",
    "                del ranges[x]        \n",
    "\n",
    "            a = []\n",
    "            for rng, cnt in ranges.items():       \n",
    "                b = {}\n",
    "                b[\"class_value\"] = str(list(rng))\n",
    "                b[\"count\"] = cnt   \n",
    "                a.append(b)    \n",
    "\n",
    "            value_array[\"counts\"] = a\n",
    "            class_label_values.append(value_array)\n",
    "            \n",
    "        feature[\"class_labels\"] =  class_label_values  \n",
    "        distribution_data.append(feature)\n",
    "             \n",
    "        return distribution_data  \n",
    "\n",
    "def compute_training_distribution(payload_df,data, request_payload , feature_attributs, feature_data_types):  \n",
    "       \n",
    "    # Sorting values of majority minority range\n",
    "    #parameters = request_payload.get(\"parameters\")\n",
    "    #training_data = request_payload.get(\"training_data\")\n",
    "    class_label = training_data_info[\"class_label\"]\n",
    "    fairness_params = { \"class_label\" : training_data_info[\"class_label\"],\"fairness_attributes\" : fairness_attributes_list }\n",
    "\n",
    "    sorted_data = {}\n",
    "    # For each feature (if available) generate the initial set of boundaries\n",
    "    bucket_size = 50\n",
    "    for feature in feature_attributs[:]:\n",
    "        values = {}\n",
    "        feature_name = feature[\"feature\"]\n",
    "        majority = feature[\"majority\"]\n",
    "        minority = feature[\"minority\"]\n",
    "        data_type = None\n",
    "\n",
    "\n",
    "        if payload_df[feature_name].dtype == np.float64 or payload_df[feature_name].dtype == np.float32 or payload_df[feature_name].dtype == np.double or payload_df[feature_name].dtype == np.longdouble:\n",
    "            data_type = \"float\"\n",
    "        elif(payload_df[feature_name].dtype == np.int64 or payload_df[feature_name].dtype == np.int32):\n",
    "            data_type = \"int\"  \n",
    "\n",
    "        if data_type is not None:\n",
    "\n",
    "            for major in majority[:]:\n",
    "                for maj in major[:]:\n",
    "                    values[maj] = maj\n",
    "            for minor in minority[:]:\n",
    "                for min in minor[:]:\n",
    "                    values[min] = min    \n",
    "            boundaries = get_boundaries(majority, minority, data_type, bucket_size)   \n",
    "            sorted_values = sorted(values.values())\n",
    "\n",
    "            feature_data = {}\n",
    "            feature_data[\"sorted_values\"] = sorted_values\n",
    "            feature_data[\"boundaries\"] = boundaries\n",
    "            sorted_data[feature_name] = feature_data\n",
    "\n",
    "\n",
    "    distribution_data = []    \n",
    "    distinct_data = {}\n",
    "    bucket_data = {}\n",
    "\n",
    "\n",
    "    #Combine all the count data under bucket as list for each fairness attributes if type is numeric ie int or float\n",
    "    for fairness_attribute in fairness_params[\"fairness_attributes\"]:\n",
    "        keys = sorted(data[fairness_attribute].keys())\n",
    "\n",
    "        is_numeric, is_float, is_int = get_data_types(keys[0])\n",
    "\n",
    "        if(feature_data_types[fairness_attribute]==\"categorical\"):\n",
    "            is_float = is_int = is_numeric = False\n",
    "\n",
    "        # No need to count data for non numeric type since buckets are for int and float datatype\n",
    "        if not is_numeric:\n",
    "            continue\n",
    "\n",
    "        bucket_dict = {}    \n",
    "        boundaries = []\n",
    "        if fairness_attribute in sorted_data:\n",
    "            boundaries = sorted_data[fairness_attribute][\"boundaries\"]\n",
    "            least_min = boundaries[0][0]\n",
    "            highest_max = boundaries[-1][1]\n",
    "            for key1, value1 in sorted(data[fairness_attribute].items()):\n",
    "                idx = -1\n",
    "                val = ast.literal_eval(str(key1))\n",
    "                val = truncate(val,5)\n",
    "                bucket = None\n",
    "                a = []\n",
    "\n",
    "                #Values less than the least minority value given as input\n",
    "                if(val<boundaries[0][0]):\n",
    "                    if(least_min!=boundaries[0][0]):\n",
    "                        bucket = str(boundaries[0])\n",
    "                        for key2, value2 in sorted(value1.items()):\n",
    "                            b = {}\n",
    "                            b[\"class_value\"] = key2\n",
    "                            b[\"count\"] = value2   \n",
    "                            a.append(b)\n",
    "\n",
    "                        existing_values = None\n",
    "                        if bucket in bucket_dict:\n",
    "                            existing_values = bucket_dict[bucket]\n",
    "                            existing_values.extend(a)\n",
    "                            del bucket_dict[bucket]\n",
    "                            boundaries [0][0] = val;\n",
    "                            bucket = str(boundaries[0])\n",
    "                            bucket_dict[bucket] = existing_values\n",
    "                    \n",
    "                    else:\n",
    "                        left_most_boundary_bucket = [val,least_min]\n",
    "                        boundaries.insert(0,left_most_boundary_bucket)\n",
    "\n",
    "                        bucket = str(boundaries[0])\n",
    "                        for key2, value2 in sorted(value1.items()):\n",
    "                            b = {}\n",
    "                            b[\"class_value\"] = key2\n",
    "                            b[\"count\"] = value2   \n",
    "                            a.append(b)\n",
    "\n",
    "                        existing_values = None\n",
    "                        if bucket in bucket_dict:\n",
    "                            existing_values = bucket_dict[bucket]\n",
    "                            existing_values.extend(a)\n",
    "                            bucket_dict[bucket] = existing_values\n",
    "                        else:\n",
    "                            bucket_dict[bucket] = a\n",
    "                        #Value found and corresponding buket has been inserted/modified so go to the next value\n",
    "                    continue;\n",
    "\n",
    "                #Values greater than the highest majority value given as input\n",
    "                if(val>boundaries[-1][1]):\n",
    "                    if(highest_max!=boundaries[-1][1]):\n",
    "                        bucket = str(boundaries[-1])\n",
    "                        for key2, value2 in sorted(value1.items()):\n",
    "                            b = {}\n",
    "                            b[\"class_value\"] = key2\n",
    "                            b[\"count\"] = value2   \n",
    "                            a.append(b)\n",
    "\n",
    "                        existing_values = None\n",
    "                        if bucket in bucket_dict:\n",
    "                            existing_values = bucket_dict[bucket]\n",
    "                            existing_values.extend(a)\n",
    "                            del bucket_dict[bucket]\n",
    "                            boundaries [-1][1] = val;\n",
    "                            bucket = str(boundaries[-1])\n",
    "                            bucket_dict[bucket] = existing_values\n",
    "                        \n",
    "                    else:\n",
    "                        right_most_boundary_bucket = [highest_max,val]\n",
    "                        boundaries.append(right_most_boundary_bucket)\n",
    "\n",
    "                        bucket = str(boundaries[-1])\n",
    "                        for key2, value2 in sorted(value1.items()):\n",
    "                            b = {}\n",
    "                            b[\"class_value\"] = key2\n",
    "                            b[\"count\"] = value2   \n",
    "                            a.append(b)\n",
    "\n",
    "                        existing_values = None\n",
    "                        if bucket in bucket_dict:\n",
    "                            existing_values = bucket_dict[bucket]\n",
    "                            existing_values.extend(a)\n",
    "                            bucket_dict[bucket] = existing_values\n",
    "                        else:\n",
    "                            bucket_dict[bucket] = a\n",
    "                    #Value found and corresponding buket has been inserted/modified so go to the next value\n",
    "                    continue;\n",
    "                \n",
    "                for boundary_bucket in boundaries[:]: \n",
    "                    idx+=1\n",
    "                    boundary_start = boundary_bucket[0]\n",
    "                    boundary_end = boundary_bucket[1]\n",
    "                    # fit the value in right boundary\n",
    "                    if(val>=boundary_start and val<=boundary_end):\n",
    "                        bucket = str(boundary_bucket)\n",
    "                        for key2, value2 in sorted(value1.items()):\n",
    "                            b = {}\n",
    "                            b[\"class_value\"] = key2\n",
    "                            b[\"count\"] = value2   \n",
    "                            a.append(b)\n",
    "\n",
    "                        existing_values = None\n",
    "                        if bucket in bucket_dict:\n",
    "                            existing_values = bucket_dict[bucket]\n",
    "                            existing_values.extend(a)\n",
    "                            bucket_dict[bucket] = existing_values\n",
    "                        else:\n",
    "                            bucket_dict[bucket] = a\n",
    "                        #Value fits in the bucket so break, no need to further loop through remaining buckets\n",
    "                        break\n",
    "\n",
    "            bucket_data[fairness_attribute] = bucket_dict        \n",
    "\n",
    "    # Sum up each of the bucket\n",
    "    bucket_summation_data = {}\n",
    "    for key, value in bucket_data.items():\n",
    "        summation_array = {}\n",
    "        for key1,value1 in value.items():\n",
    "            bucket_data_count = {}\n",
    "            for val in value1[:]:\n",
    "                class_value = val[\"class_value\"]\n",
    "                count = val[\"count\"]\n",
    "                if not class_value in bucket_data_count:\n",
    "                    bucket_data_count[class_value] = count\n",
    "                else:\n",
    "                    bucket_data_count[class_value] = bucket_data_count[class_value] + count  \n",
    "            summation_array[key1] = bucket_data_count\n",
    "        bucket_summation_data[key] =  summation_array\n",
    "\n",
    "\n",
    "    distribution_data = []    \n",
    "    distinct_data = {}\n",
    "\n",
    "    #Build the json\n",
    "    for fairness_attribute in fairness_params[\"fairness_attributes\"]:\n",
    "\n",
    "        keys = sorted(data[fairness_attribute].keys())\n",
    "\n",
    "        min = None\n",
    "        max = None\n",
    "\n",
    "        is_numeric, is_float, is_int = get_data_types(keys[0])\n",
    "\n",
    "        if(feature_data_types[fairness_attribute]==\"categorical\"):\n",
    "            is_float = is_int = is_numeric = False\n",
    "\n",
    "        key_values = None\n",
    "        if len(keys)>0:\n",
    "            if is_numeric:\n",
    "                if is_float:\n",
    "                    key_values = sorted(list(map(float, keys)))\n",
    "                else:\n",
    "                    key_values = sorted(list(map(int, keys)))  \n",
    "\n",
    "                min = key_values[0]\n",
    "                max = key_values[len(key_values)-1]\n",
    "\n",
    "        feature = {}\n",
    "        feature[\"attribute\"] = fairness_attribute\n",
    "        if is_numeric:\n",
    "            feature[\"min\"] = min\n",
    "            feature[\"max\"] = max\n",
    "        else:    \n",
    "            feature[\"distinct_values\"] = keys    \n",
    "\n",
    "        class_label_values = []\n",
    "        if not is_numeric:     \n",
    "            for key1, value1 in sorted(data[fairness_attribute].items()):\n",
    "                value_array = {}\n",
    "                value_array[\"label\"] = key1\n",
    "                #value_array[\"total_rows\"] = total_rows\n",
    "\n",
    "                a = []\n",
    "                for key2, value2 in sorted(value1.items()):\n",
    "                    b = {}\n",
    "                    b[\"class_value\"] = key2\n",
    "                    b[\"count\"] = value2   \n",
    "                    a.append(b)\n",
    "                value_array[\"counts\"] = a\n",
    "                class_label_values.append(value_array)\n",
    "        else:\n",
    "            bucket_data = None\n",
    "            if fairness_attribute in bucket_summation_data and len(keys)>bucket_size:\n",
    "                bucket_data = bucket_summation_data[fairness_attribute]            \n",
    "            else:\n",
    "                bucket_data = data[fairness_attribute] \n",
    "\n",
    "            for key,value in bucket_data.items():\n",
    "                value_array = {}\n",
    "                #value_array[\"label\"] = key\n",
    "                value_array[\"label\"] = key\n",
    "                a = []\n",
    "                for key2, value2 in sorted(value.items()):\n",
    "                    b = {}\n",
    "                    b[\"class_value\"] = key2\n",
    "                    b[\"count\"] = value2   \n",
    "                    a.append(b)\n",
    "                value_array[\"counts\"] = a\n",
    "                class_label_values.append(value_array)\n",
    "\n",
    "        feature[\"class_labels\"] =  class_label_values  \n",
    "        distribution_data.append(feature)\n",
    "\n",
    "    return distribution_data\n",
    " \n",
    "def get_data_types(val):\n",
    "     \n",
    "    is_numeric = False\n",
    "    is_int = False\n",
    "    is_float = False\n",
    "    try:\n",
    "        float(val)\n",
    "        is_numeric = True\n",
    "        is_float = True\n",
    "    except ValueError:\n",
    "        pass\n",
    " \n",
    "    try:\n",
    "        import unicodedata\n",
    "        unicodedata.numeric(val)\n",
    "        is_numeric = True\n",
    "        is_float = True\n",
    "    except (TypeError, ValueError):\n",
    "        pass\n",
    "    \n",
    "    try:\n",
    "        int(val)\n",
    "        is_numeric = True\n",
    "        is_int = True\n",
    "    except ValueError:\n",
    "        pass\n",
    "           \n",
    "    return is_numeric, is_float, is_int\n",
    "\n",
    "def get_boundaries(majority, minority, data_type,bucket_size):\n",
    "    values = {}\n",
    "    boundaries = []\n",
    "    if data_type is not None:    \n",
    "        for major in majority[:]:\n",
    "            for maj in major[:]:\n",
    "                values[maj] = maj\n",
    "        for minor in minority[:]:\n",
    "            for min in minor[:]: \n",
    "                values[min] = min\n",
    "\n",
    "        sorted_values = sorted(values.values())\n",
    "\n",
    "        min = sorted_values[0]\n",
    "        max = sorted_values[-1]\n",
    "        diff = max - min\n",
    "\n",
    "        range_cnt = diff/bucket_size\n",
    "        range = None\n",
    "\n",
    "        if not data_type is None and data_type==\"int\":    \n",
    "            range = int(range_cnt)\n",
    "        else:    \n",
    "            range = truncate(range_cnt,5)\n",
    "\n",
    "        temp = min\n",
    "        distribution = {}\n",
    "        output = {}\n",
    "        start = min\n",
    "        end = max\n",
    "\n",
    "        # Create the initial boundaries\n",
    "        length_sorted_values = len(sorted_values)\n",
    "        idx = 0\n",
    "\n",
    "        if not data_type is None and data_type==\"int\":\n",
    "            next_start = temp\n",
    "        else:\n",
    "            next_start = truncate(temp,5)\n",
    "\n",
    "        while temp <= max:\n",
    "            #start = round(temp,1)\n",
    "            if not data_type is None and data_type==\"int\":\n",
    "                start = next_start\n",
    "                end = temp+range\n",
    "\n",
    "                if (idx<(length_sorted_values)):\n",
    "\n",
    "                    if(sorted_values[idx] == end):\n",
    "                        idx+=1\n",
    "                        next_start = temp+range\n",
    "                        boundary = [start,end]\n",
    "                        boundaries.append(boundary)\n",
    "                        temp = temp + range\n",
    "\n",
    "                    elif(sorted_values[idx] == start):\n",
    "                        idx+=1\n",
    "                        while(sorted_values[idx]<end):\n",
    "                            boundary = [start,sorted_values[idx]]\n",
    "                            boundaries.append(boundary)\n",
    "                            start = sorted_values[idx]\n",
    "                            idx+=1\n",
    "\n",
    "                        next_start = temp+range\n",
    "                        temp = temp + range\n",
    "                        boundary = [start,end]\n",
    "                        boundaries.append(boundary)   \n",
    "\n",
    "                    elif(sorted_values[idx]>=start and sorted_values[idx]<=end):\n",
    "                        while(idx<len(sorted_values) and sorted_values[idx]<end):\n",
    "                            start_diff = sorted_values[idx] - start\n",
    "                            end_diff = end - sorted_values[idx]\n",
    "\n",
    "                            if start_diff>end_diff:\n",
    "                                boundary = [start,sorted_values[idx]]\n",
    "                                boundaries.append(boundary)\n",
    "                                start = sorted_values[idx]\n",
    "                                idx+=1\n",
    "\n",
    "                            else:\n",
    "                                boundary = [start,sorted_values[idx]]\n",
    "                                boundaries.append(boundary)\n",
    "                                start = sorted_values[idx]\n",
    "                                idx+=1\n",
    "\n",
    "                        next_start = temp+range\n",
    "                        temp = temp + range\n",
    "                        boundary = [start,end]\n",
    "                        boundaries.append(boundary)       \n",
    "\n",
    "                    else:\n",
    "                        next_start = temp+range\n",
    "                        boundary = [start,end]\n",
    "                        boundaries.append(boundary)\n",
    "                        temp = temp + range\n",
    "\n",
    "                else:\n",
    "                    next_start = temp+range\n",
    "                    boundary = [start,end]\n",
    "                    boundaries.append(boundary)\n",
    "                    temp = temp + range\n",
    "\n",
    "            else:\n",
    "                start = next_start  \n",
    "                end = truncate(temp+range,5)\n",
    "\n",
    "                if (idx<(length_sorted_values)):\n",
    "                    sorted_value = truncate(sorted_values[idx],5)\n",
    "\n",
    "                    if(sorted_value == end):\n",
    "                        idx+=1\n",
    "                        next_start = truncate(temp+range,5)\n",
    "                        boundary = [start,end]\n",
    "                        boundaries.append(boundary)\n",
    "                        temp = temp + range\n",
    "\n",
    "                    elif(sorted_value == start):\n",
    "                        idx+=1\n",
    "                        if(idx<length_sorted_values):\n",
    "                            sorted_value = truncate(sorted_values[idx],5)\n",
    "                            while(sorted_value<end and idx<len(sorted_values)):\n",
    "                                boundary = [start,sorted_value]\n",
    "                                boundaries.append(boundary)\n",
    "                                start = sorted_value\n",
    "                                idx+=1\n",
    "                                if(idx<length_sorted_values):\n",
    "                                    sorted_value = truncate(sorted_values[idx],5)\n",
    "\n",
    "                        next_start = truncate(temp+range,5)\n",
    "                        temp = temp + range\n",
    "                        boundary = [start,end]\n",
    "                        boundaries.append(boundary)   \n",
    "\n",
    "                    elif(sorted_value>=start and sorted_value<=end):\n",
    "                        while(sorted_value<end and idx<len(sorted_values)):    \n",
    "                            start_diff = sorted_value - start\n",
    "                            end_diff = end - sorted_value\n",
    "\n",
    "                            if start_diff>end_diff:\n",
    "                                boundary = [start,sorted_value]\n",
    "                                boundaries.append(boundary)\n",
    "                                start = sorted_value\n",
    "                                idx+=1\n",
    "                                if(idx<length_sorted_values):\n",
    "                                    sorted_value = truncate(sorted_values[idx],5)\n",
    "\n",
    "                            else:\n",
    "                                boundary = [start,sorted_value]\n",
    "                                boundaries.append(boundary)\n",
    "                                start = sorted_value\n",
    "                                idx+=1\n",
    "                                if(idx<len(sorted_values)):\n",
    "                                    sorted_value = truncate(sorted_values[idx],5)\n",
    "\n",
    "                        next_start = truncate(temp+range,5)\n",
    "                        temp = temp + range\n",
    "                        boundary = [start,end]\n",
    "                        boundaries.append(boundary)       \n",
    "\n",
    "                    else:\n",
    "                        next_start = truncate(temp+range,5)\n",
    "                        boundary = [start,end]\n",
    "                        boundaries.append(boundary)\n",
    "                        temp = temp + range\n",
    "\n",
    "                else:\n",
    "                    next_start = truncate(temp+range,5)\n",
    "                    boundary = [start,end]\n",
    "                    boundaries.append(boundary)\n",
    "                    temp = temp + range\n",
    "\n",
    "    return boundaries \n",
    "\n",
    "def getDistinctValues(dataset,inputs):  \n",
    "\n",
    "    distinct_values_sync = {}\n",
    "    attributes_dataset = dataset[inputs['fairness_attributes']]\n",
    "\n",
    "    for protected_attribute in inputs['fairness_attributes']:\n",
    "\n",
    "        protected_attribute_column =  attributes_dataset[protected_attribute].tolist()\n",
    "        is_column_numeric = False\n",
    "        count_numeric = 0\n",
    "        row_num = attributes_dataset[protected_attribute].shape[0]\n",
    "\n",
    "        if row_num<1000:\n",
    "            sample_size = row_num\n",
    "        else:\n",
    "            sample_size = 1000\n",
    "\n",
    "        for i in range(sample_size):\n",
    "            protected_attribute_column_value = protected_attribute_column[random.randint(0,row_num-1)]\n",
    "            is_numeric, is_float, is_int = get_data_types(protected_attribute_column_value)\n",
    "            if is_numeric or is_float or is_int :\n",
    "                count_numeric+=1\n",
    "\n",
    "        if count_numeric > sample_size/2:\n",
    "            is_column_numeric = True\n",
    "\n",
    "        protected_attribute_column_sorted = custom_sorted(protected_attribute_column)\n",
    "\n",
    "        if  is_column_numeric:\n",
    "            min = protected_attribute_column_sorted[0]\n",
    "            for pos in range(1,row_num):\n",
    "                is_numeric, is_float, is_int = get_data_types(protected_attribute_column_sorted[-pos]) \n",
    "                if is_numeric or is_float or is_int :\n",
    "                    max = protected_attribute_column_sorted[-pos]\n",
    "                    break\n",
    "            distinct_protected_attribute_column = [min,max]\n",
    "            distinct_values_sync[protected_attribute] = distinct_protected_attribute_column\n",
    "\n",
    "        else:\n",
    "            distinct_protected_attribute_column = set()\n",
    "            for attribute_value in protected_attribute_column_sorted:\n",
    "                is_numeric, is_float, is_int = get_data_types(attribute_value)\n",
    "                if not(is_numeric or is_float or is_int):\n",
    "                    distinct_protected_attribute_column.add(attribute_value)\n",
    "\n",
    "            distinct_values_sync[protected_attribute] = list(distinct_protected_attribute_column)\n",
    "\n",
    "    return distinct_values_sync \n",
    "\n",
    "def truncate(float_value, number_of_digits):\n",
    "    return math.floor(float_value * 10 ** number_of_digits) / 10 ** number_of_digits           \n",
    "\n",
    "def custom_sorted(l):\n",
    "    try:\n",
    "        convert = lambda text: int(text) if text.isdigit() else text\n",
    "        alphanum_key = lambda key: [convert(c) for c in re.split('([0-9]+)', key)]\n",
    "        return sorted(l, key = alphanum_key) \n",
    "    except:\n",
    "        return sorted(l)      \n",
    "\n",
    "def cleanPayloadData(payload_df,fairness_params):\n",
    "\n",
    "    #Get the numerical/categorical percentage \n",
    "    attributes_dataset = payload_df[fairness_params['fairness_attributes']]\n",
    "    total_rows = payload_df.shape[0]\n",
    "    feature_data_types = {}\n",
    "\n",
    "    for protected_attribute in fairness_params['fairness_attributes']:\n",
    "        row_num = payload_df.shape[0]\n",
    "        protected_attribute_column = payload_df[protected_attribute].tolist()\n",
    "        is_column_numeric = False\n",
    "        count_numeric = 0\n",
    "\n",
    "        for idx in range(row_num-1):\n",
    "            protected_attribute_column_value = protected_attribute_column[idx]\n",
    "            is_numeric, is_float, is_int = get_data_types(protected_attribute_column_value)\n",
    "            if is_numeric or is_float or is_int :\n",
    "                count_numeric+=1\n",
    "\n",
    "        #Clean the data\n",
    "        if count_numeric >= (.98*total_rows):\n",
    "            indexes_to_drop = []\n",
    "            for idx in range(row_num):\n",
    "                protected_attribute_column_value = attributes_dataset[protected_attribute][idx]\n",
    "                is_numeric, is_float, is_int = get_data_types(protected_attribute_column_value)\n",
    "                if not(is_numeric or is_float or is_int):\n",
    "                    indexes_to_drop.append(idx)\n",
    "\n",
    "            feature_data_types[protected_attribute] = \"numerical\"\n",
    "            payload_df = payload_df.drop(payload_df.index[indexes_to_drop])\n",
    "\n",
    "        elif count_numeric <=(.02*total_rows):\n",
    "            indexes_to_drop = []\n",
    "            for idx in range(row_num-1):\n",
    "                protected_attribute_column_value = protected_attribute_column[idx]\n",
    "                is_numeric, is_float, is_int = get_data_types(protected_attribute_column_value)\n",
    "                if is_numeric or is_float or is_int:\n",
    "                    indexes_to_drop.append(idx)\n",
    "\n",
    "            feature_data_types[protected_attribute] = \"categorical\"\n",
    "            payload_df = payload_df.drop(payload_df.index[indexes_to_drop])\n",
    "\n",
    "        else:\n",
    "            raise Exception(\"Improper input data provided in \" + protected_attribute + \" column\") \n",
    "\n",
    "    return payload_df, feature_data_types    \n",
    "\n",
    "bias_training_distribution = computeTrainingDataDistribution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Following cell contains code to generate training data distribution json for explainability :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Cell-6: Responsible for generating explainability service distribution \n",
    "\n",
    "!pip install lime\n",
    "# -----------------------------------------------------------------------------\n",
    "#  Copyright (c) 2016, Marco Tulio Correia Ribeiro All rights reserved\n",
    "# -----------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "import collections\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from lime.discretize import QuartileDiscretizer\n",
    "\n",
    "feature_cols=training_data_info[\"feature_columns\"]\n",
    "categorical_cols=training_data_info[\"categorical_columns\"]\n",
    "label_col=training_data_info[\"class_label\"]\n",
    "numeric_cols = list(set(feature_cols) ^ set(categorical_cols))\n",
    "\n",
    "# Convert columns to numeric incase data frame read them as non-numeric\n",
    "data_df[numeric_cols] = data_df[numeric_cols].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# Drop rows with invalid values\n",
    "data_df.dropna(axis=\"index\", subset=feature_cols, inplace=True)\n",
    "\n",
    "random_state=10\n",
    "\n",
    "training_data_schema = list(data_df.columns.values)\n",
    "training_data_shape = data_df.shape[1]\n",
    "\n",
    "# Feature column index\n",
    "feature_column_index = [training_data_schema.index(x) for x in feature_cols]\n",
    "\n",
    "# Categorical columns index\n",
    "categorical_column_index = []\n",
    "categorical_column_index = [feature_cols.index(x) for x in categorical_cols]\n",
    "\n",
    "# numeric columns\n",
    "numeric_column_index = []\n",
    "index = 0\n",
    "for f_col_index in feature_column_index :\n",
    "    if index not in categorical_column_index:\n",
    "        numeric_column_index.append(index)\n",
    "    index = index + 1\n",
    "\n",
    "# class labels\n",
    "class_labels = []\n",
    "if model_type != \"regression\":\n",
    "    if(label_col != None):\n",
    "        class_labels = data_df[label_col].unique()\n",
    "        class_labels = class_labels.tolist()\n",
    "\n",
    "\n",
    "# Filter feature columns from training data frames\n",
    "data_frame = data_df.values\n",
    "data_frame_features = data_frame[:, feature_column_index]\n",
    "\n",
    "# Compute stats on complete training data\n",
    "data_frame_num_features = data_frame_features[:,numeric_column_index]\n",
    "num_base_values = np.median(data_frame_num_features,axis=0)\n",
    "stds = np.std(data_frame_num_features, axis=0, dtype=\"float64\")\n",
    "mins = np.min(data_frame_num_features, axis=0)\n",
    "maxs = np.max(data_frame_num_features, axis=0)\n",
    "\n",
    "main_base_values = {}\n",
    "main_cat_counts = {}\n",
    "if(len(categorical_column_index) > 0):\n",
    "    for cat_col in categorical_column_index:\n",
    "        cat_col_value_counts = Counter(data_frame_features[:, cat_col])\n",
    "        values, frequencies = map(list, zip(*(cat_col_value_counts.items())))\n",
    "        max_freq_index = frequencies.index(np.max(frequencies))\n",
    "        cat_base_value = values[max_freq_index]\n",
    "        main_base_values[cat_col] = cat_base_value\n",
    "        main_cat_counts[cat_col] = cat_col_value_counts\n",
    "\n",
    "num_feature_range = np.arange(len(numeric_column_index))\n",
    "main_stds = {}\n",
    "main_mins = {}\n",
    "main_maxs = {}\n",
    "for x in num_feature_range:\n",
    "    index = numeric_column_index[x]\n",
    "    main_base_values[index] = num_base_values[x]\n",
    "    main_stds[index] = stds[x]\n",
    "    main_mins[index] = mins[x]\n",
    "    main_maxs[index] = maxs[x]\n",
    "    \n",
    "# Encode categorical columns\n",
    "categorical_columns_encoding_mapping = {}\n",
    "for column_index_to_encode in categorical_column_index:\n",
    "    le = LabelEncoder()\n",
    "    le.fit(data_frame_features[:, column_index_to_encode])\n",
    "    data_frame_features[:, column_index_to_encode] = le.transform(\n",
    "        data_frame_features[:, column_index_to_encode])\n",
    "    categorical_columns_encoding_mapping[column_index_to_encode] = le.classes_\n",
    "\n",
    "\n",
    "# Compute training stats on descritized data\n",
    "descritizer = QuartileDiscretizer(\n",
    "    data_frame_features, categorical_features=categorical_column_index, feature_names=feature_cols, labels=class_labels, random_state= random_state)\n",
    "\n",
    "d_means = descritizer.means\n",
    "d_stds = descritizer.stds\n",
    "d_mins = descritizer.mins\n",
    "d_maxs = descritizer.maxs\n",
    "d_bins = descritizer.bins(data_frame_features, labels=class_labels)\n",
    "\n",
    "# Compute feature values and frequencies of all columns\n",
    "cat_features = np.arange(data_frame_features.shape[1])\n",
    "discretized_training_data = descritizer.discretize(data_frame_features)\n",
    "\n",
    "feature_values = {}\n",
    "feature_frequencies = {}\n",
    "for feature in cat_features:\n",
    "    column = discretized_training_data[:, feature]\n",
    "    feature_count = collections.Counter(column)\n",
    "    values, frequencies = map(list, zip(*(feature_count.items())))\n",
    "    feature_values[feature] = values\n",
    "    feature_frequencies[feature] = frequencies\n",
    "\n",
    "index = 0\n",
    "d_bins_revised = {}\n",
    "for bin in d_bins:\n",
    "    d_bins_revised[numeric_column_index[index]] = bin.tolist()\n",
    "    index = index + 1\n",
    "\n",
    "#Encode categorical columns\n",
    "cat_col_mapping = {}\n",
    "for column_index_to_encode in categorical_column_index:\n",
    "    cat_col_encoding_mapping_value = categorical_columns_encoding_mapping[column_index_to_encode]\n",
    "    cat_col_mapping[column_index_to_encode] = cat_col_encoding_mapping_value.tolist()\n",
    "\n",
    "\n",
    "# Construct stats\n",
    "data_stats = {}\n",
    "data_stats[\"feature_columns\"] = feature_cols\n",
    "data_stats[\"categorical_columns\"] = categorical_cols\n",
    "\n",
    "#Common\n",
    "data_stats[\"feature_values\"] = feature_values\n",
    "data_stats[\"feature_frequencies\"] = feature_frequencies\n",
    "data_stats[\"class_labels\"] = class_labels\n",
    "data_stats[\"categorical_columns_encoding_mapping\"] = cat_col_mapping\n",
    "\n",
    "#Descritizer\n",
    "data_stats[\"d_means\"] = d_means\n",
    "data_stats[\"d_stds\"] = d_stds\n",
    "data_stats[\"d_maxs\"] = d_maxs\n",
    "data_stats[\"d_mins\"] = d_mins\n",
    "data_stats[\"d_bins\"] = d_bins_revised\n",
    "\n",
    "#Full data\n",
    "data_stats[\"base_values\"] = main_base_values\n",
    "data_stats[\"stds\"] = main_stds\n",
    "data_stats[\"mins\"] = main_mins\n",
    "data_stats[\"maxs\"] = main_maxs\n",
    "data_stats[\"categorical_counts\"] = main_cat_counts\n",
    "\n",
    "\n",
    "#Convert to json\n",
    "exp_cofig = {}\n",
    "for k in data_stats:\n",
    "    key_details = data_stats.get(k)\n",
    "    if(key_details is not None) and (not isinstance(key_details, list)):\n",
    "        new_details = {}\n",
    "        for key_in_details in key_details:\n",
    "            new_details[str(key_in_details)] = key_details[key_in_details]\n",
    "    else :\n",
    "        new_details = key_details\n",
    "    exp_cofig[k] = new_details\n",
    "    \n",
    "\n",
    "#print(exp_cofig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell-7: Build the final json\n",
    "\n",
    "#Fairness final configuration\n",
    "fairness_config_json = {}\n",
    "parameters_json = {}\n",
    "feature_json = fairness_attributes\n",
    "parameters_json[\"features\"] = feature_json\n",
    "\n",
    "parameters_json[\"favourable_class\"] = parameters[\"favourable_class\"]\n",
    "parameters_json[\"unfavourable_class\"] = parameters[\"unfavourable_class\"]\n",
    "parameters_json[\"min_records\"] = min_records\n",
    "parameters_json[\"model_type\"] = model_type\n",
    "\n",
    "fairness_config_json[\"parameters\"] = parameters_json\n",
    "fairness_config_json[\"distributions\"] = bias_training_distribution\n",
    "\n",
    "#Set input data schema in common configuration\n",
    "common_configuration = {}\n",
    "common_configuration[\"problem_type\"] = model_type\n",
    "common_configuration[\"label_column\"] = training_data_info.get(\"class_label\")\n",
    "common_configuration[\"input_data_schema\"] = training_schema\n",
    "\n",
    "\n",
    "#Add common Bias and explainabiity data distribution to main json\n",
    "d = {}\n",
    "d[\"fairness_configuration\"] = fairness_config_json\n",
    "d[\"common_configuration\"] = common_configuration\n",
    "d[\"explainability_configuration\"] = exp_cofig\n",
    "\n",
    "json_data = json.dumps(d,indent=2)\n",
    "\n",
    "# optionally write json to a file and create a download link\n",
    "\n",
    "f = open(\"training_distribution.json\",\"w+\")\n",
    "f.write(json_data)\n",
    "f.close()\n",
    "print(\"Finished writing data to training_distribution.json\")\n",
    "\n",
    "def create_download_link( title = \"Download training data distribution JSON file\", filename = \"training_distribution.json\"):  \n",
    "    \n",
    "    b64 = base64.b64encode(json_data.encode())\n",
    "    payload = b64.decode()\n",
    "    html = '<a download=\"{filename}\" href=\"data:text/json;base64,{payload}\" target=\"_blank\">{title}</a>'\n",
    "    html = html.format(payload=payload,title=title,filename=filename)\n",
    "    return HTML(html)\n",
    "\n",
    "create_download_link()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
